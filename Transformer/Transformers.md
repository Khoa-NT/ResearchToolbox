# People

## [Lucas Beyer](https://lucasb.eyer.be/#home) (Twitter: [@lucasbey](https://x.com/giffmana))
- Transformer tutorial slides (must-read): [Link](http://lucasb.eyer.be/transformer)
- [Talks Collection](https://lucasb.eyer.be/snips/talks.html)


<br />

# Explaination / Visualization
- [Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html) from [Sebastian Raschka](https://sebastianraschka.com/)
    <details><summary>Click to show image</summary> 

    ![alt text](src/Sebastian_Raschka_1.png)
    </details>

<br />

- [Transformer Explainer](https://poloclub.github.io/transformer-explainer/)
    <details><summary>Click to show image</summary>

    ![alt text](src/image.png)
    </details>

<br />

- [Transformer Circuits Thread](https://transformer-circuits.pub/)
    <details><summary>Click to show image</summary> 

    ![alt text](src/image-1.png) 
    </details>

<br />

- [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
    <details><summary>Click to show image</summary> 

    ![alt text](src/image-2.png)
    </details>

<br />

# Practice
- [Transformer Puzzles](https://github.com/srush/Transformer-Puzzles) from [Professor Alexander Rush](https://rush-nlp.com/). Also check out his [Puzzle collection](https://github.com/srush?tab=repositories&q=Puzzles&type=&language=&sort=).
